#!/usr/bin/env perl

# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

# code borrows extensively from at least the following:

# https://github.com/apache/incubator-mxnet/blob/master/perl-package/AI-MXNet/examples/gluon/mnist.pl
# http://blogs.perl.org/users/sergey_kolychev/2017/10/machine-learning-in-perl-part3-deep-convolutional-generative-adversarial-network.html

use strict;
use warnings;
use AI::MXNet qw(mx);
use AI::MXNet::Gluon qw(gluon);
use AI::MXNet::AutoGrad qw(autograd);
use AI::MXNet::Gluon::NN qw(nn);
use AI::MXNet::Base;
use Getopt::Long qw(HelpMessage);
use Data::Dumper;

use feature 'say';

GetOptions(
    'lr=f'           => \( my $lr           = 0.001 ),
    'log-interval=i' => \( my $log_interval = 20 ),
    'momentum=f'     => \( my $momentum     = 0.9 ),
    'hybridize=i'    => \( my $hybridize    = 0 ),
    'cuda=i'         => \( my $cuda         = 0 ),
    'load_params=i'  => \( my $load_params  = 0 ),
    'batch-size=i'   => \( my $batch_size   = 1 ),
    'epochs=i'       => \( my $epochs       = 20 ),
    'info=i'         => \( my $info        = 0 ),
    'help' => sub { HelpMessage(0) },
) or HelpMessage(1);

my $ctx = $cuda ? mx->gpu(0) : mx->cpu;

# define network

my $net = nn->Sequential();
$net->name_scope(
    sub {
        $net->add(
            nn->Conv2D( channels => 6, kernel_size => 5, activation => 'relu' )
        );
        $net->add( nn->MaxPool2D( pool_size => 2, strides => 2 ) );
        $net->add(
            nn->Conv2D(
                channels    => 16,
                kernel_size => 3,
                activation  => 'relu'
            )
        );
        $net->add( nn->MaxPool2D( pool_size => 2, strides => 2 ) );
        $net->add( nn->Flatten() );
        $net->add( nn->Dense( 120, activation => "relu" ) );
        $net->add( nn->Dense( 84,  activation => "relu" ) );
        $net->add( nn->Dense(2) );
    }
);

#$net->name_scope(sub {
#    $net->add(nn->Dense(128, activation=>'relu'));
#    $net->add(nn->Dense(64, activation=>'relu'));
#    $net->add(nn->Dense(2));
#});

$net->hybridize() if $hybridize;
$net->load_parameters('xmit.params') if $load_params;

# data

sub transformer {
    my ( $data, $label ) = @_;

    # put channel first
    $data = $data->transpose( [ 2, 0, 1 ] );  # change to channel, height, width
    #say Dumper($data->shape);
    $data = $data->astype('float32') / 255.0;

    return ( $data, $label );
}

my $train_dataset = gluon->data->vision->ImageFolderDataset(
    root      => './training/train',
    flag      => 1,
    transform => \&transformer
);
my $train_data = gluon->data->DataLoader(
    $train_dataset,
    batch_size => $batch_size,
    shuffle    => 1,
    last_batch => 'discard'
);

my $val_dataset = gluon->data->vision->ImageFolderDataset(
    root      => './training/test',
    flag      => 1,
    transform => \&transformer
);
my $val_data = gluon->data->DataLoader(
    $val_dataset,
    batch_size => $batch_size,
    shuffle    => 0
);

&train_info if $info;

sub train_info {

    my $sample = $train_data->[0];
    my $data   = $sample->[0];
    my $label  = $sample->[1];
    say sprintf( 'data type: %s label type: %s', $data->dtype, $label->dtype );
    say Dumper( $data->at(0)->shape );

    #say Dumper($train_dataset->items);
    say sprintf(
        'sample image name: %s, label: %s',
        $train_dataset->items->[0][0],
        $train_dataset->items->[0][1]
    );
    say sprintf( 'batch size: %s', $data->len );
    say 'labels:';
    foreach my $label ( @{ $train_dataset->synsets } ) {
        say sprintf( 'label: %s', $label );
    }
    say sprintf( 'total training data: %d',   $train_dataset->len );
    say sprintf( 'total validation data: %d', $val_dataset->len );

    #print Dumper($train_data->[0][0]->dtype);
    #print Dumper($train_data);
    #print $train_data->[0][0]->aspdl;

    dump_all_images($train_data);
    exit;
}

sub write_image {
    my ($data, $location) = @_;

    $data = $data->at(0)->aspdl;

    if ($data->getdim(2) == 3) {                 # color:
        $data = $data->reorder(2, 0, 1)->         #  RGB values first
                        slice('x', 'x', '-1:0');  #  invert image
    } else {                                     # grayscale:
        $data = $data->squeeze->                  #  remove uneeded dimension
                        slice('x', '-1:0');       #  invert image
    }

    my $image = ( $data * 255 )->byte;

    $image->wpic($location);
}

sub dump_all_images {
    my $set_ref = shift;

    my @set = @$set_ref;
    mkdir "image_dump";
    for my $i ( 0 .. $#set ) {
        my $data  = ${ $set[$i] }[0];
        #my $label = ${ $set[$i] }[1];

        write_image($data, "image_dump/$i.png");
    }
}

sub visualize {
    my ( $batch, $iter ) = @_;
    mkdir "data_images";
    mkdir "data_images/$iter";

    for my $i ( 0 .. $batch_size - 1 ) {

        write_image($batch, "data_images/$iter/$i.png");

    }
}

sub get_mislabeled {
    my $loader = shift;

    mkdir "mislabeled";

    my @tendl = @$loader;

    my @text_labels = ( 'data', 'voice' );

    my $topline;
    $topline .= '  PREDICTION :: CORRECT' . "\n";
    $topline .= '  ========== :: =======' . "\n";

    print $topline;

    for my $i ( 0 .. $#tendl ) {
        my $data  = ${ $tendl[$i] }[0];
        my $label = ${ $tendl[$i] }[1];

        my $ot   = $net->($data)->argmax( { axis => 1 } );
        my $pred = $text_labels[ PDL::sclr( $ot->aspdl ) ];
        my $true = $text_labels[ PDL::sclr( $label->aspdl ) ];

        my $otline;
        $otline .= sprintf( "%12s",  $pred ) . " :: ";
        $otline .= sprintf( "%-10s", $true ) . " ";
        #$otline .= ( $pred eq $true ) ? ".." : "XX";
	if ( $pred eq $true ) {
	    $otline .= "..";
	} else {
	    $otline .= "XX";
	    my $hashish =  int($data->aspdl->sum);
            write_image($data, sprintf("mislabeled/%s-%s.png", $hashish, $true));
	}

        print $otline . "\n";

    }
}

sub test {
    my $ctx    = shift;
    my $metric = mx->metric->Accuracy();
    while ( defined( my $d = <$val_data> ) ) {
        my ( $data, $label ) = @$d;
        $data  = $data->as_in_context($ctx);
        $label = $label->as_in_context($ctx);
        my $output = $net->($data);
        $metric->update( [$label], [$output] );
    }
    return $metric->get;
}

sub train {
    my ( $epochs, $ctx ) = @_;

    # Collect all parameters from net and its children, then initialize them.
    $net->initialize( mx->init->Xavier( magnitude => 2.24 ), ctx => $ctx );

    # Trainer is for updating parameters with gradient.
    my $trainer = gluon->Trainer( $net->collect_params(),
        'sgd', { learning_rate => $lr, momentum => $momentum } );
    my $metric = mx->metric->Accuracy();
    my $loss   = gluon->loss->SoftmaxCrossEntropyLoss();

    for my $epoch ( 0 .. $epochs - 1 ) {

	## set scalars to hold time and mean loss
	my $time = time();
	my $llm;

        # reset data iterator and metric at begining of epoch.
        $metric->reset();
        my $data;
        my $label;
        enumerate(
            sub {
                my ( $i, $d ) = @_;
                ( $data, $label ) = @$d;
                $data  = $data->as_in_context($ctx);
                $label = $label->as_in_context($ctx);

                # Start recording computation graph with record() section.
                # Recorded graphs can then be differentiated with backward.
                my $output;
		my $L;
                autograd->record(
                    sub {
                        $output = $net->($data);
                        $L = $loss->( $output, $label );
                        $L->backward;
                    }
                );

		## capture the mean loss
		$llm = PDL::sclr($L->mean->aspdl );

                # take a gradient step with batch_size equal to data.shape[0]
                $trainer->step( $data->shape->[0] );

                # update metric at last.
                $metric->update( [$label], [$output] );

                if ( $i % $log_interval == 0 and $i > 0 ) {
                    my ( $name, $acc ) = $metric->get();
                    print "[Epoch $epoch Batch $i] Training: $name=$acc\n";
                }
            },
            \@{$train_data}
        );

        my ( $trn_name, $trn_acc ) = $metric->get();
        #print "[Epoch $epoch] Training: $name=$acc\n";

        #get_mislabeled($train_data);
        visualize( $data, $epoch );

        my ( $val_name, $val_acc ) = test($ctx);
        #print "[Epoch $epoch] Validation: $val_name=$val_acc\n";

	## capture time interval
	my $nowtime = time();
	my $interval = $nowtime - $time;

        say sprintf('Epoch %d: loss %.3f, train acc %.3f, test acc %.3f in %0.1f secs',
		    $epoch, $llm, $trn_acc, $val_acc, $interval);

        #get_mislabeled($val_data);
    }
    $net->save_parameters('xmit.params');
}

train( $epochs, $ctx );
